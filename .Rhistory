SOTU_fre_sent <- data.frame("year" = SOTU_year, "SOTU_stat" = SOTU_stat$meanSentenceLength)
SOTU_fre_word <- data.frame("year" = SOTU_year, "SOTU_stat" = SOTU_stat$meanWordSyllables)
###EO
data("data_corpus_eo")
#drop the ones that are empty
clean1 <- data_corpus_eo$documents
##problems
clean1$texts<-stri_replace_all_fixed(clean1$texts, "A. D.", "AD")
#drop anything that's very long (95th percentile and above)
quant95 <- which( nchar(clean1$texts) >
quantile( nchar(clean1$texts), prob=.95) )
clean2 <- clean1[-quant95, c(1:2)]
eo_corp <- corpus(clean2, text_field = "texts")
##drop very short sentences
eo_corp <- corpus_trimsentences(eo_corp, min_length = 4)
eo_corp <- corpus_subset(eo_corp, ntoken(eo_corp) > 10)
eo_year <- docvars(eo_corp)
eo_stat <- textstat_readability(eo_corp$documents$texts, measure = c("Flesch", "meanSentenceLength", "meanWordSyllables"))
eo_fre_df <- data.frame("year" = eo_year$Year, "eo_stat" = eo_stat$Flesch)
eo_fre_sent <- data.frame("year" = eo_year$Year, "eo_stat" = eo_stat$meanSentenceLength)
eo_fre_word <- data.frame("year" = eo_year$Year, "eo_stat" = eo_stat$meanWordSyllables)
########SCOTUS
#load("C:/Users/kevin/Documents/GitHub/BMS_chapter_replication/data/data_corpus_SCOTUS.rda")
load("C:/Users/kevin/Desktop/data_corpus_SCOTUS.rda")
#drop the ones that are empty
clean1 <- corpus_subset(data_corpus_SCOTUS, nchar(texts(data_corpus_SCOTUS)) > 0)
## implement fixes
texts(clean1) <- stri_replace_all_fixed(texts(clean1), "U. S.", "US")
#drop anything that's very long (95th percentile and above)
temp_lengths <- stri_length(texts(clean1))
clean2 <- corpus_subset(clean1, temp_lengths <quantile(temp_lengths, prob = .95))
scotus_lengths<-ntoken(clean2, removePunct = T)
clean3 <- corpus_trimsentences(clean2, min_length = 4)
scotus_year <- clean3$documents$Year
scotus_stat <- textstat_readability(clean3$documents$texts, measure = c("Flesch", "meanSentenceLength", "meanWordSyllables"))
scotus_fre_df <- data.frame("year" = scotus_year, "scotus_stat" = scotus_stat$Flesch)
scotus_fre_sent <- data.frame("year" = scotus_year, "scotus_stat" = scotus_stat$meanSentenceLength)
scotus_fre_word <- data.frame("year" = scotus_year, "scotus_stat" = scotus_stat$meanWordSyllables)
##too many to plot well, and very unbalanced (way more modern ones)--try for a more balanced sample
years<-unique(scotus_fre_df$year)
indices<-vector()
for(i in 1:length(years)){
set<-which(scotus_fre_df[,"year"]==years[i])
num<-min(length(set), 30)
samp<-sample(set, num, replace = FALSE)
indices<-c(indices, samp)
}
balanced_scotus_fre_df<-scotus_fre_df[indices,]
balanced_scotus_fre_sent<-scotus_fre_sent[indices,]
balanced_scotus_fre_word<-scotus_fre_word[indices,]
require(reshape2)
###Congress
load("C:/Users/kevin/Desktop/data_corpus_CR.rdata")
#drop the ones that are by the speaker
speaker <- which( (data_corpus_CR$documents$name)=="Speaker" )
clean1 <- data_corpus_CR$documents[-speaker, ]
#drop anything that's very long (90th percentile and above)
quant95 <- which( nchar(clean1$texts) >
quantile( nchar(clean1$texts), prob=.95) )
clean2 <- clean1[-quant95, ]
##sync up year format
clean2$year[clean2$year==95]<-1995
clean2$year[clean2$year==96]<-1996
clean2$year[clean2$year==97]<-1997
clean2$year[clean2$year==98]<-1998
clean2$year[clean2$year==99]<-1999
clean2$year[clean2$year==0]<-2000
clean2$year[clean2$year==1]<-2001
clean2$year[clean2$year==2]<-2002
clean2$year[clean2$year==3]<-2003
clean2$year[clean2$year==4]<-2004
clean2$year[clean2$year==5]<-2005
clean2$year[clean2$year==6]<-2006
clean2$year[clean2$year==7]<-2007
clean2$year[clean2$year==8]<-2008
##take a very small sample--can't run readability on the whole thing
sample<-sample(seq(1,length(clean2$texts)), 10000, replace = F  )
sample_data<-clean2[sample,]
congress_year <- sample_data$year
congress_stat <- textstat_readability(sample_data$texts, measure = c("Flesch", "meanSentenceLength", "meanWordSyllables"))
congress_fre_df <- data.frame("year" = congress_year, "congress_stat" = congress_stat$Flesch)
congress_fre_sent <- data.frame("year" = congress_year, "congress_stat" = congress_stat$meanSentenceLength)
congress_fre_word <- data.frame("year" = congress_year, "congress_stat" = congress_stat$meanWordSyllables)
##melt together for plotting--Figure 3
require(reshape2)
df_US<-melt(list(SOTU=SOTU_fre_df, SCOTUS=balanced_scotus_fre_df, Congress = congress_fre_df,
ExecOrders = eo_fre_df), id.vars="year")
require(ggplot2)
linez<-c( "F1", "dashed",  "dotdash","solid")
p <- ggplot(data = df_US,
aes(x = year, y = value, linetype = L1)) + #, group = delivery)) +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.line = element_line(colour = "black")) +
geom_smooth(alpha=0.2,  method = "loess", span = .34, color = "black", se = F) +
coord_cartesian(ylim = c(-20, 65)) +
theme(legend.position="none", axis.text.x = element_text(size = 15),
axis.text.y = element_text(size = 15), axis.title.y = element_text(size = 15))+
scale_linetype_manual(values = linez)+
xlab("") +
ylab("Flesch Reading Ease Score") +
theme(plot.title = element_text(lineheight=.8, face="bold")) +
annotate("text", label = "Congress Speech", x = 1995, y = 45, size = 6, colour = "black")+
annotate("text", label = "SOTU", x = 1800, y = 20, size = 6, colour = "black")+
annotate("text", label = "SCOTUS", x = 1810, y = 50, size = 6, colour = "black") +
annotate("text", label = "Executive Orders", x = 1970, y = 0, size = 6, colour = "black")
print(p)
##melt together for plotting--Figure 5
df_US_sent<-melt(list(SOTU=SOTU_fre_sent, SCOTUS=balanced_scotus_fre_sent, Congress = congress_fre_sent,
ExecOrders = eo_fre_sent), id.vars="year")
linez<-c( "F1", "dashed",  "dotdash","solid")
p <- ggplot(data = df_US_sent,
aes(x = year, y = value, linetype = L1)) + #, group = delivery)) +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.line = element_line(colour = "black")) +
geom_smooth(alpha=0.2,  method = "loess", span = .34, color = "black", se = F) +
coord_cartesian(ylim = c(15, 65)) +
theme(legend.position="none", axis.text.x = element_text(size = 15),
axis.text.y = element_text(size = 15), axis.title.y = element_text(size = 15))+
scale_linetype_manual(values = linez)+
xlab("") +
ylab("Mean Sentence Length") +
theme(plot.title = element_text(lineheight=.8, face="bold")) +
annotate("text", label = "Congress Speech", x = 1960, y = 17, size = 6, colour = "black")+
annotate("text", label = "SOTU", x = 1910, y = 25, size = 6, colour = "black")+
annotate("text", label = "SCOTUS", x = 1810, y = 30, size = 6, colour = "black") +
annotate("text", label = "Executive Orders", x = 1960, y = 50, size = 6, colour = "black")
print(p)
##melt together for plotting--Figure 7
df_US_word<-melt(list(SOTU=SOTU_fre_word, SCOTUS=balanced_scotus_fre_word, Congress = congress_fre_word,
ExecOrders = eo_fre_word), id.vars="year")
p <- ggplot(data = df_US_word,
aes(x = year, y = value, linetype = L1)) + #, group = delivery)) +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.background = element_blank(),
axis.line = element_line(colour = "black")) +
geom_smooth(alpha=0.2,  method = "loess", span = .34, color = "black", se = F) +
coord_cartesian(ylim = c(1.4, 1.85)) +
theme(legend.position="none", axis.text.x = element_text(size = 15),
axis.text.y = element_text(size = 15), axis.title.y = element_text(size = 15))+
scale_linetype_manual(values = linez)+
xlab("") +
ylab("Mean Number of Syllables per Word") + theme(plot.title = element_text(lineheight=.8, face="bold")) +
annotate("text", label = "SOTU", x = 1800, y = 1.73, size = 6, colour = "black")+
annotate("text", label = "SCOTUS", x = 1810, y = 1.47, size = 6, colour = "black") +
annotate("text", label = "Con. Speech", x = 1998, y = 1.61, size = 6, colour = "black")+
annotate("text", label = "Executive Orders", x = 1950, y = 1.82, size = 6, colour = "black")
print(p)
##Figures 4, 6 and 8: Comparative data
data("data_corpus_SOTU")
SOTU_stat <- textstat_readability(data_corpus_SOTU, measure = c("Flesch", "meanSentenceLength", "meanWordSyllables"))
SOTU_year <- lubridate::year(docvars(data_corpus_SOTU, "Date"))
SOTU_fre_df <- data.frame("year" = SOTU_year, "SOTU_stat" = SOTU_stat$Flesch)
SOTU_fre_sent <- data.frame("year" = SOTU_year, "SOTU_stat" = SOTU_stat$meanSentenceLength)
SOTU_fre_word <- data.frame("year" = SOTU_year, "SOTU_stat" = SOTU_stat$meanWordSyllables)
########nobel
#load("data_text/NobelLitePresentations/data_corpus_nobel.rdata")
load("C:/Users/kevin/Desktop/data_corpus_nobel.rdata")
temp_lengths <- stri_length(texts(data_corpus_nobel))
data_corpus_nobel <- corpus_subset(data_corpus_nobel, temp_lengths < quantile(temp_lengths, prob = .95))
nobel_corp <- corpus_trimsentences(data_corpus_nobel, min_length = 4)
nobel_lengths<-ntoken(nobel_corp, removePunct = T)
nobel_stat <- textstat_readability(nobel_corp, measure = c("Flesch", "meanSentenceLength", "meanWordSyllables"))
nobel_year <- lubridate::year(docvars(data_corpus_nobel, "Date"))
load("C:/Users/kevin/Desktop/data_corpus_nobel.rdata")
temp_lengths <- stri_length(texts(data_corpus_nobel))
data_corpus_nobel <- corpus_subset(data_corpus_nobel, temp_lengths < quantile(temp_lengths, prob = .95))
nobel_corp <- corpus_trimsentences(data_corpus_nobel, min_length = 4)
nobel_lengths<-ntoken(nobel_corp, removePunct = T)
nobel_stat <- textstat_readability(nobel_corp, measure = c("Flesch", "meanSentenceLength", "meanWordSyllables"))
nobel_year <- lubridate::year(docvars(data_corpus_nobel, "Date"))
docvars(data_corpus_nobel, "Date")
docvars(data_corpus_nobel)
nobel_year <- lubridate::year(docvars(data_corpus_nobel, "year"))
nobel_year <- (docvars(data_corpus_nobel, "year"))
install.packages("randomizr", repos="http://r.declaredesign.org")
library("randomizr", lib.loc="~/R/win-library/3.4")
remove.packages("randomizr", lib="~/R/win-library/3.4")
install.packages("randomizr", repos="http://r.declaredesign.org")
install.packages("randomizr", repos = "http://r.declaredesign.org")
install.packages("randomizr", repos="http://r.declaredesign.org")
install.packages("randomizr")
library("randomizr", lib.loc="~/R/win-library/3.4")
library(DeclareDesign)
library(quanteda)
library(sophistication)
data("data_corpus_Crimson")
library(dplyr)
require(stringr)
require(data.table)
##read in
load("C:/Users/kevin/Documents/GitHub/sophistication-papers/analysis_article/output/fitted_BT_model.Rdata")
results<-predict_readability(BT_best, newdata = data_corpus_SOTU, bootstrap_n = 10, verbose = T)
data(data_corpus_sotu, package = "quanteda.corpora")
data("data_corpus_SOTU")
library(quanteda)
install.packages("quanteda")
install.packages("quanteda")
library(quanteda)
data_corpus_SOTU
results<-predict_readability(BT_best, newdata = data_corpus_SOTU, bootstrap_n = 10, verbose = T)
library(sophistication)
results<-predict_readability(BT_best, newdata = data_corpus_SOTU, bootstrap_n = 10, verbose = T)
data(data_corpus_sotu, package = "quanteda.corpora")
library(quanteda.quanteda)
library(quanteda.corpora)
install.packages("quanteda.corpora")
devtools::install_github("quanteda/quanteda.corpora")
devtools::install_github("quanteda/quanteda.corpora")
devtools::install_github("quanteda/quanteda.corpora", force = TRUE)
install.packages("digest")
library(readtext)
xx<-readtext(xx.txt)
xx<-readtext("xx.txt")
xx<-readtext(file ="xx.txt")
?readtext
xx<-readtext(file ="xx")
setwd("C:/Users/kevin/Documents/GitHub/YT_mapping_replication/")
yt_sj<-read.csv("data/youtube_sj_100.csv", header = T, stringsAsFactors = F)
yt_sj$month<-substr(yt_sj$video_publish_date, 1, 7)
yt_feminism<-read.csv("data/youtube_feminism_100.csv", header = T, stringsAsFactors = F)
yt_feminism$month<-substr(yt_feminism$video_publish_date, 1, 7)
yt_white<-read.csv("data/youtube_white_100.csv", header = T, stringsAsFactors = F)
yt_white$month<-substr(yt_white$video_publish_date, 1, 7)
ain<-read.csv(file = 'data/AIN_channel_stats.csv', header = TRUE, stringsAsFactors = F)
msm<-read.csv(file = 'data/media_yt.csv',  header = TRUE, stringsAsFactors = F)
msm<-read.csv(file = 'data/media_yt.csv',  header = TRUE, stringsAsFactors = F)
library(tidyverse)
yt_sj_ain<-filter(yt_sj, channel_id %in% ain$id)
yt_sj_msm<-filter(yt_sj, channel_id %in% msm$channel_id)
yt_feminism_ain<-filter(yt_feminism, channel_id %in% ain$id)
yt_feminism_msm<-filter(yt_feminism, channel_id %in% msm$channel_id)
yt_white_ain<-filter(yt_white, channel_id %in% ain$id)
yt_white_msm<-filter(yt_white, channel_id %in% msm$channel_id)
months<-yt_feminism$month
months<-unique(yt_feminism$month)
for(i in 1:length(months)){
apropos<-filter(yt_sj_ain, month == months[i])
sj_ain_density[i]<-length(apropos$channel_title)
apropos<-filter(yt_sj_msm, month == months[i])
sj_msm_density[i]<-length(apropos$channel_title)
apropos<-filter(yt_feminism_ain, month == months[i])
feminism_ain_density[i]<-length(apropos$channel_title)
apropos<-filter(yt_feminism_msm, month == months[i])
feminism_msm_density[i]<-length(apropos$channel_title)
apropos<-filter(yt_white_ain, month == months[i])
white_ain_density[i]<-length(apropos$channel_title)
apropos<-filter(yt_white_msm, month == months[i])
white_msm_density[i]<-length(apropos$channel_title)
}
###initialize
sj_ain_density<-vector()
sj_msm_density<-vector()
feminism_msm_density<-vector()
feminism_ain_density<-vector()
white_msm_density<-vector()
white_ain_density<-vector()
##for each month
months<-unique(yt_feminism$month)
for(i in 1:length(months)){
apropos<-filter(yt_sj_ain, month == months[i])
sj_ain_density[i]<-length(apropos$channel_title)
apropos<-filter(yt_sj_msm, month == months[i])
sj_msm_density[i]<-length(apropos$channel_title)
apropos<-filter(yt_feminism_ain, month == months[i])
feminism_ain_density[i]<-length(apropos$channel_title)
apropos<-filter(yt_feminism_msm, month == months[i])
feminism_msm_density[i]<-length(apropos$channel_title)
apropos<-filter(yt_white_ain, month == months[i])
white_ain_density[i]<-length(apropos$channel_title)
apropos<-filter(yt_white_msm, month == months[i])
white_msm_density[i]<-length(apropos$channel_title)
}
alldens<-data.frame(months, feminism_msm_density, feminism_ain_density, white_msm_density, white_ain_density,
sj_msm_density, sj_ain_density)
alldens.m = melt(alldens, id.vars ="months", measure.vars = c("feminism_msm_density","feminism_ain_density", "white_msm_density","white_ain_density",
"sj_msm_density","sj_ain_density"))
library(reshape2)
ggplot(alldens.m, aes(y=value, x=months, group=variable, color = variable)) +
geom_smooth(se = F, span = .2, size = 2)+ scale_x_discrete(breaks=c("2008-01","2009-01","2010-01",
"2011-01", "2012-01", "2013-01",
"2014-01", "2015-01", "2016-01",
"2017-01", "2018-01"))+
geom_point() +
xlab(NULL) + ylab("Frequency")  + theme_bw() + ggtitle("Videos in the Top 100 per Month") +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.text.x = element_text(angle = 45,
size = 14))
alldens.m = melt(alldens, id.vars ="months", measure.vars = c("feminism_msm_density","feminism_ain_density", "white_msm_density","white_ain_density",
"sj_msm_density","sj_ain_density"))
ggplot(alldens.m, aes(y=value, x=months, group=variable, color = variable)) +
geom_smooth(se = F, span = .2, size = 2)+ scale_x_discrete(breaks=c("2008-01","2009-01","2010-01",
"2011-01", "2012-01", "2013-01",
"2014-01", "2015-01", "2016-01",
"2017-01", "2018-01"))+
geom_point() +
xlab(NULL) + ylab("Frequency")  + theme_bw() + ggtitle("Videos in the Top 100 per Month") +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.text.x = element_text(angle = 45,
size = 14))
sj_ain<-vector()
sj_msm<-vector()
feminism_msm<-vector()
feminism_ain<-vector()
white_msm<-vector()
white_ain<-vector()
for(i in 1:132){
apropos<-filter(yt_sj_ain, month == months[i])
sj_ain[i]<-length(apropos$channel_title)
apropos<-filter(yt_sj_msm, month == months[i])
sj_msm[i]<-length(apropos$channel_title)
apropos<-filter(yt_feminism_ain, month == months[i])
feminism_ain[i]<-length(apropos$channel_title)
apropos<-filter(yt_feminism_msm, month == months[i])
feminism_msm[i]<-length(apropos$channel_title)
apropos<-filter(yt_white_ain, month == months[i])
white_ain[i]<-length(apropos$channel_title)
apropos<-filter(yt_white_msm, month == months[i])
white_msm[i]<-length(apropos$channel_title)
}
alldens<-data.frame(months, feminism_msm, feminism_ain, white_msm, white_ain,
sj_msm, sj_ain)
alldens.m = melt(alldens, id.vars ="months", measure.vars = c("feminism_msm","feminism_ain", "white_msm","white_ain",
"sj_msm","sj_ain"))
ggplot(alldens.m, aes(y=value, x=months, group=variable, color = variable)) +
geom_smooth(se = F, span = .2, size = 2)+ scale_x_discrete(breaks=c("2008-01","2009-01","2010-01",
"2011-01", "2012-01", "2013-01",
"2014-01", "2015-01", "2016-01",
"2017-01", "2018-01"))+
geom_point() +
xlab(NULL) + ylab("Frequency")  + theme_bw() + ggtitle("Videos in the Top 100 per Month") +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.text.x = element_text(angle = 45,
size = 14))
alldens.m = melt(alldens, id.vars ="months", measure.vars = c("feminism_msm","white_msm","sj_msm","feminism_ain", "white_ain",
"sj_ain"))
ggplot(alldens.m, aes(y=value, x=months, group=variable, color = variable)) +
geom_smooth(se = F, span = .2, size = 2)+ scale_x_discrete(breaks=c("2008-01","2009-01","2010-01",
"2011-01", "2012-01", "2013-01",
"2014-01", "2015-01", "2016-01",
"2017-01", "2018-01"))+
geom_point() +
xlab(NULL) + ylab("Frequency")  + theme_bw() + ggtitle("Videos in the Top 100 per Month: Niche Topics") +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.text.x = element_text(angle = 45,
size = 14))
ggplot(alldens.m, aes(y=value, x=months, group=variable, color = variable)) +
geom_smooth(se = F, span = .2, size = 2)+ scale_x_discrete(breaks=c("2008-01","2009-01","2010-01",
"2011-01", "2012-01", "2013-01",
"2014-01", "2015-01", "2016-01",
"2017-01", "2018-01"))+
geom_point() +
xlab(NULL) + ylab("Frequency")  + theme_bw() + ggtitle("Videos in the Top 100 per Month: Niche Topics") +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.text.x = element_text(angle = 45,
size = 14), legend.title = element_blank())
pdf("figures/frequency_niche_topics.pdf", 7, 5)
ggplot(alldens.m, aes(y=value, x=months, group=variable, color = variable)) +
geom_smooth(se = F, span = .2, size = 2)+ scale_x_discrete(breaks=c("2008-01","2009-01","2010-01",
"2011-01", "2012-01", "2013-01",
"2014-01", "2015-01", "2016-01",
"2017-01", "2018-01"))+
geom_point() +
xlab(NULL) + ylab("Frequency")  + theme_bw() + ggtitle("Videos in the Top 100 per Month: Niche Topics") +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.text.x = element_text(angle = 45,
size = 14), legend.title = element_blank())
dev.off()
library(tidyverse)
library(reshape2)
setwd("C:/Users/kevin/Documents/GitHub/YT_mapping_replication/")
setwd("C:/Users/kevin/Documents/GitHub/YT_mapping_replication/")
yt_economy<-read.csv("data/youtube_economy_100_US.csv", header = T, stringsAsFactors = F)
yt_economy$month<-substr(yt_economy$video_publish_date, 1, 7)
yt_news<-read.csv("data/youtube_news_100.csv", header = T, stringsAsFactors = F)
yt_economy<-read.csv("data/youtube_economy_100.csv", header = T, stringsAsFactors = F)
yt_economy$month<-substr(yt_economy$video_publish_date, 1, 7)
###news
yt_news<-read.csv("data/youtube_news_100.csv", header = T, stringsAsFactors = F)
yt_news$month<-substr(yt_news$video_publish_date, 1, 7)
###politics
yt_politics<-read.csv("data/youtube_politics_100.csv", header = T, stringsAsFactors = F)
yt_politics$month<-substr(yt_politics$video_publish_date, 1, 7)
#####now  AIN // MSM channels
ain<-read.csv(file = 'data/AIN_channel_stats.csv', header = TRUE, stringsAsFactors = F)
msm<-read.csv(file = 'data/media_yt.csv',  header = TRUE, stringsAsFactors = F)
####
##restrict to matches
yt_economy_ain<-filter(yt_economy, channel_id %in% ain$id)
yt_economy_msm<-filter(yt_economy, channel_id %in% msm$channel_id)
yt_news_ain<-filter(yt_news, channel_id %in% ain$id)
yt_news_msm<-filter(yt_news, channel_id %in% msm$channel_id)
yt_politics_ain<-filter(yt_politics, channel_id %in% ain$id)
yt_politics_msm<-filter(yt_politics, channel_id %in% msm$channel_id)
###initialize
economy_ain<-vector()
economy_msm<-vector()
news_msm<-vector()
news_ain<-vector()
politics_msm<-vector()
politics_ain<-vector()
##for each month
months<-unique(yt_news$month)
for(i in 1:length(months)){
apropos<-filter(yt_economy_ain, month == months[i])
economy_ain[i]<-length(apropos$channel_title)
apropos<-filter(yt_economy_msm, month == months[i])
economy_msm[i]<-length(apropos$channel_title)
apropos<-filter(yt_news_ain, month == months[i])
news_ain[i]<-length(apropos$channel_title)
apropos<-filter(yt_news_msm, month == months[i])
news_msm[i]<-length(apropos$channel_title)
apropos<-filter(yt_politics_ain, month == months[i])
politics_ain[i]<-length(apropos$channel_title)
apropos<-filter(yt_politics_msm, month == months[i])
politics_msm[i]<-length(apropos$channel_title)
}
alldens<-data.frame(months, news_msm, news_ain, politics_msm, politics_ain,
economy_msm, economy_ain)
alldens.m = melt(alldens, id.vars ="months", measure.vars = c("news_msm","politics_msm","economy_msm","news_ain", "politics_ain",
"economy_ain"))
ggplot(alldens.m, aes(y=value, x=months, group=variable, color = variable)) +
geom_smooth(se = F, span = .2, size = 2)+ scale_x_discrete(breaks=c("2008-01","2009-01","2010-01",
"2011-01", "2012-01", "2013-01",
"2014-01", "2015-01", "2016-01",
"2017-01", "2018-01"))+
geom_point() +
xlab(NULL) + ylab("Frequency")  + theme_bw() + ggtitle("Videos in the Top 100 per Month: Major Topics") +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.text.x = element_text(angle = 45,
size = 14), legend.title = element_blank())
pdf("figures/frequency_major_topics.pdf", 7, 5)
ggplot(alldens.m, aes(y=value, x=months, group=variable, color = variable)) +
geom_smooth(se = F, span = .2, size = 2)+ scale_x_discrete(breaks=c("2008-01","2009-01","2010-01",
"2011-01", "2012-01", "2013-01",
"2014-01", "2015-01", "2016-01",
"2017-01", "2018-01"))+
geom_point() +
xlab(NULL) + ylab("Frequency")  + theme_bw() + ggtitle("Videos in the Top 100 per Month: Major Topics") +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), axis.text.x = element_text(angle = 45,
size = 14), legend.title = element_blank())
dev.off()
setwd("C:/Users/kevin/Dropbox/youtube/analysis/")
options(scipen=10000)
meta<-read.csv("data/vid_meta_msm.csv", header = T, stringsAsFactors = F)
setwd("C:/Users/kevin/Documents/GitHub/YT_mapping_replication/")
meta<-read.csv("data/vid_meta_msm.csv", header = T, stringsAsFactors = F)
meta<-filter(meta, is.na(meta$video_view_count) == F & is.na(meta$video_like_count) == F & is.na(meta$video_dislike_count) == F &
is.na(meta$video_comment_count) == F )
meta<-meta %>% distinct(video_id, .keep_all = TRUE)
meta<-filter(meta, month >= "2006-08-01" & month < "2018-11-01")
meta$month<-as.Date(paste(substr(meta$video_publish_date, 1, 7),"-01",sep=""))
meta<-filter(meta, month >= "2006-08-01" & month < "2018-11-01")
meta$video_view_count <- as.numeric(meta$video_view_count)
summary(meta$video_view_count)
meta<-filter(meta, is.na(meta$video_view_count) == F & is.na(meta$video_like_count) == F & is.na(meta$video_dislike_count) == F &
#         is.na(meta$video_comment_count) == F )
####drop duplicated video_id's -- lose 2k vids here
#meta<-meta %>% distinct(video_id, .keep_all = TRUE)
##I ran this scraper midway through November 2018, so it doesn't count as a full month (lose 30k vids here)
#meta$month<-as.Date(paste(substr(meta$video_publish_date, 1, 7),"-01",sep=""))
#meta<-filter(meta, month >= "2006-08-01" & month < "2018-11-01")
summary(meta$video_view_count)
meta<-select(meta, video_view_count, video_like_count , video_comment_count , month)
##calculate video categories
table(meta$video_category)/length(meta$video_id)
sum_likes<-aggregate(video_like_count ~ dates, data = meta,
sum)
pdf(file = "sum_likes_msm.pdf", width = 8, height = 5)
qplot( sum_likes$dates, sum_likes$video_like_count, geom = 'smooth' )+ theme(
plot.background = element_rect(fill = "white"),
panel.background = element_rect(fill = "white"),
axis.line.x = element_line(color = "grey")
) +  ylab("Sum Likes per Day") + ylim(0,NA) +ggtitle("MSM Likes Per Day ")
dev.off()
mean_views<-aggregate(video_view_count ~ dates, data = meta,
mean)
sum_views<-aggregate(video_view_count ~ dates, data = meta,
sum)
pdf(file = "sum_views_msm.pdf", width = 8, height = 5)
#plot(mean_views$dates , (mean_views$video_view_count), xlab = "Date", ylab = "Mean Views", col = "red")
qplot( sum_views$dates, sum_views$video_view_count, geom = 'smooth' ) + theme(
plot.background = element_rect(fill = "white"),
panel.background = element_rect(fill = "white"),
axis.line.x = element_line(color = "grey")
) + xlab("Date") + ylab("Sum Views per Day") + ylim(0,NA) +ggtitle("MSM Views Per Day ")
dev.off()
#scatter.smooth(as.Date(mean_views$dates), (mean_views$video_view_count), col = "red", span = .1)
vids_ain[148]
vids_msm<-table(meta$month)
vids_msm_short<-vids_msm
#vids_msm_short<-vids_msm[9:156]
vids_msm_short[vids_msm_short>15000]<-15000
pdf(file = "vids_msm_short.pdf", width = 8, height = 5)
plot(vids_msm_short,  ylab = "Videos Uploaded per Month", main = "Videos by MSM")
meta<-filter(meta, is.na(meta$video_view_count) == F & is.na(meta$video_like_count) == F & is.na(meta$video_dislike_count) == F &
is.na(meta$video_comment_count) == F )
meta<-select(meta, video_view_count, video_like_count , video_comment_count , month)
write.csv(file ="data/vid_meta_msm_preprocessed.csv", x = meta)
meta<-read.csv("data/vid_meta_msm.csv", header = T, stringsAsFactors = F)
meta<-read.csv("data/vid_meta_msm_preprocessed.csv", header = T, stringsAsFactors = F)
table(meta$video_category)/length(meta$video_id)
sum_likes<-aggregate(video_like_count ~ month, data = meta,
sum)
qplot( sum_likes$dates, sum_likes$video_like_count, geom = 'smooth' )+ theme(
plot.background = element_rect(fill = "white"),
panel.background = element_rect(fill = "white"),
axis.line.x = element_line(color = "grey")
) +  ylab("Sum Likes per Month") + ylim(0,NA) +ggtitle("MSM Likes Per Month ")
qplot( sum_likes$month, sum_likes$video_like_count, geom = 'smooth' )+ theme(
plot.background = element_rect(fill = "white"),
panel.background = element_rect(fill = "white"),
axis.line.x = element_line(color = "grey")
) +  ylab("Sum Likes per Month") + ylim(0,NA) +ggtitle("MSM Likes Per Month ")
View(sum_likes)
